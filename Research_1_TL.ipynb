{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ynKoRLWuiT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "BaKRfknPYe07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.misc\n",
        "from skimage import io\n",
        "import pandas as pd\n",
        "import os\n",
        "from scipy.ndimage import gaussian_filter"
      ],
      "metadata": {
        "id": "X0Pzb-y30cOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square_padding (image):\n",
        "  original_height, original_width = image.shape[0], image.shape[1]\n",
        "\n",
        "  # Calculate the amount of padding needed to make the image a square\n",
        "  pad_height = max(0, original_width - original_height)\n",
        "  pad_width = max(0, original_height - original_width)\n",
        "\n",
        "  # Pad the image with 0s to make it a square\n",
        "  padded_image = np.pad(image, ((0, pad_height), (0, pad_width), (0, 0)), 'constant')\n",
        "  return padded_image"
      ],
      "metadata": {
        "id": "8kpMcNoYiQ_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_meanAndVaraince(arr):\n",
        "  tensor = tf.convert_to_tensor(arr)\n",
        "\n",
        "# Compute the mean and variance of the Tensor\n",
        "  meanVal = tf.math.reduce_mean(tensor)\n",
        "  varianceVal = tf.math.reduce_variance(tensor)\n",
        "  return [meanVal,varianceVal]"
      ],
      "metadata": {
        "id": "w-b6HqwSi6NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_image_reader(csv_file, image_dir, trainingCond,output_shape = (224,224)):\n",
        "    data = pd.read_csv(csv_file)\n",
        "    mrnNumbers = data.iloc[:,0]\n",
        "    eyePosition =  data.iloc[:,1]\n",
        "    gestational_Age = data.iloc[:,3]\n",
        "    weights = data.iloc[:,4]\n",
        "    zoneValue = data.iloc[:,6]\n",
        "    print('zoneValue')\n",
        "    print(zoneValue)\n",
        "    print('end')\n",
        "    stageValue = data.iloc[:,7]\n",
        "    PlusValue = data.iloc[:,8]\n",
        "    \n",
        "    num_of_mrns = len(mrnNumbers)\n",
        "    \n",
        "    FAImages = []\n",
        "    fundusImages = []\n",
        "    age = []\n",
        "    weight = []\n",
        "\n",
        "    zone = []\n",
        "    stage= []\n",
        "    plus= []\n",
        "    maxm =0;\n",
        "    for idx in range(0,num_of_mrns):\n",
        "       print('idx=',idx)\n",
        "       pathFA = str(mrnNumbers[idx])+'/'+'FALeft'\n",
        "       pathFundus = str(mrnNumbers[idx])+'/'+'fundusLeft'\n",
        "       if eyePosition[idx]=='R':\n",
        "        pathFA = str(mrnNumbers[idx])+'/'+'FARight'\n",
        "        pathFundus = str(mrnNumbers[idx])+'/'+'fundusRight'\n",
        "       pathFA = os.path.join(image_dir,pathFA)\n",
        "       pathFundus = os.path.join(image_dir,pathFundus)\n",
        "       #print(pathFA)\n",
        "       #print(pathFundus)\n",
        "       for images1 in os.listdir(pathFA):\n",
        "          #if maxm == 10:\n",
        "           #break\n",
        "          if (images1.endswith(\".jpg\")):\n",
        "            for images2 in os.listdir(pathFundus):\n",
        "              #if maxm == 10:\n",
        "               # break\n",
        "              if (images2.endswith(\".jpg\")):\n",
        "                #print(images1)\n",
        "                #print(images2)\n",
        "                pathImages1 = os.path.join(pathFA+'/'+images1)\n",
        "                pathImages2 = os.path.join(pathFundus+'/'+images2)\n",
        "                #print(pathImages1)\n",
        "                #print(pathImages2)\n",
        "                faImage = cv2.imread(pathImages1)\n",
        "                fundusImage = cv2.imread(pathImages2)\n",
        "                #print(faImage)\n",
        "                #print(fundusImage)\n",
        "                faImage1 = square_padding(faImage)\n",
        "                faImage1 = cv2.resize(faImage, output_shape)\n",
        "               \n",
        "                fundusImage1 = square_padding(fundusImage)\n",
        "                fundusImage1 = cv2.resize(fundusImage1, output_shape)\n",
        "                fundusImages.append(fundusImage1)\n",
        "                FAImages.append(fundusImage1)\n",
        "                age.append(gestational_Age[idx])\n",
        "                weight.append(weights[idx])\n",
        "                zone.append(zoneValue[idx])\n",
        "                stage.append(stageValue[idx])\n",
        "                plus.append(PlusValue[idx])  \n",
        "                maxm = maxm+1              \n",
        "                                            \n",
        "    FAImages = np.array(FAImages)\n",
        "    FAImages =FAImages/255\n",
        "    fundusImages = np.array(fundusImages)\n",
        "    fundusImages = fundusImages/255\n",
        "    age = np.array(age)\n",
        "    weight = np.array(weight)\n",
        "    zone = np.array(zone)\n",
        "    \n",
        "    print('zoneee')\n",
        "    print(zone)\n",
        "    print('endl')\n",
        "    stage = np.array(stage)\n",
        "    \n",
        "    #if trainingCond == 'training':\n",
        "    zone = to_categorical(zone, num_classes=4)\n",
        "    stage = to_categorical(stage, num_classes=5)\n",
        "    print('zoneee')\n",
        "    print(zone)\n",
        "    print('endl')\n",
        "    \n",
        "    plus = np.array(plus)\n",
        "    print(stage.shape)\n",
        "    dataset = {'FAImages': FAImages, 'fundusImages': fundusImages, 'age':age, 'weight': weight, 'zone':zone, 'stage':stage, 'plus':plus}  \n",
        "    print(len(plus))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "WQD9GoNk0jhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = training_image_reader('/content/drive/MyDrive/Data/Train/ROPTrainingData.csv', '/content/drive/MyDrive/Data/Train/Images','training', output_shape = (224,224))\n"
      ],
      "metadata": {
        "id": "dS__9QSg0z4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "TXrVIChY5Hvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "CtbzbM2c6XPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pickle.dumps(dataloader)\n",
        "\n",
        "# Write the byte stream to a file\n",
        "with open('dataset.pkl', 'wb') as f:\n",
        "  f.write(data)"
      ],
      "metadata": {
        "id": "v1PF-Eoo4iqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_weight=0.5\n",
        "zero_weight=0.5\n",
        "from keras import backend as ke"
      ],
      "metadata": {
        "id": "foKZofiGz9q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_binary_crossentropy(y_true, y_pred):\n",
        "  b_ce = ke.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "        # weighted calc\n",
        "  weight_vector = y_true * one_weight + (1 - y_true) * zero_weight\n",
        "  weighted_b_ce = weight_vector * b_ce\n",
        "  return ke.mean(weighted_b_ce)"
      ],
      "metadata": {
        "id": "KbfEdCJ2ynrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FAImages = dataloader['FAImages']\n",
        "fundusImages = dataloader['fundusImages']\n",
        "age = dataloader['age']\n",
        "\n",
        "meanAge = 25.584545454545456\n",
        "varianceAge = 1.850775580110642\n",
        "age = (age - meanAge) / varianceAge\n",
        "weight = dataloader['weight']\n",
        "meanWeight = 0.6219090909090909\n",
        "varianceWeight = 0.12644111424651727\n",
        "weight = (weight-meanWeight)/varianceWeight\n",
        "zone = dataloader['zone']\n",
        "stage = dataloader['stage']\n",
        "plus = dataloader['plus']\n",
        "medical_data = np.stack((age,weight), axis = 1)"
      ],
      "metadata": {
        "id": "o8tOn7hq1lbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [224, 224]"
      ],
      "metadata": {
        "id": "t_-tEasExUf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first input model\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(2, input_dim=2, activation=\"relu\"))\n",
        "#model1.add(Normalization(mean=0.5, variance=1.0))\n",
        "model1.add(Dense(4, activation=\"relu\"))\n",
        "\n",
        "#Second input model\n",
        "visible2 = Input(shape=(224,224,3))\n",
        "conv_base2 = VGG16(include_top=False,weights='imagenet', input_shape=IMAGE_SIZE + [3])\n",
        "for layer in conv_base2.layers:\n",
        "    layer._name = layer._name + str('_D')\n",
        "conv_base2.trainable = False ## Not trainable weights\n",
        "layerFirst = GlobalAveragePooling2D()(conv_base2.output)\n",
        "flat2 = Flatten(name=\"flatten1\")(layerFirst)\n",
        "flat2 = Dense(256, activation = \"relu\")(flat2)\n",
        "flat2 = Dense(64, activation = \"relu\")(flat2)\n",
        "flat2 = Dense(16, activation = \"relu\")(flat2)\n",
        "\n",
        "# third input model\n",
        "\n",
        "conv_base3 = VGG16(include_top=False,weights='imagenet', input_shape=IMAGE_SIZE + [3])\n",
        "conv_base3._name=\"NAME\"\n",
        "conv_base3.trainable = False ## Not trainable weights\n",
        "layerSecond = GlobalAveragePooling2D()(conv_base3.output)\n",
        "flat3 = Flatten(name=\"flatten2\")(layerSecond )\n",
        "flat3 = Dense(256, activation = \"relu\")(flat3)\n",
        "flat3 = Dense(64, activation = \"relu\")(flat3)\n",
        "flat3 = Dense(16, activation = \"relu\")(flat3)\n",
        "\n",
        "mergeImages = concatenate([flat2,flat3],name=\"Images_Combine\")\n",
        "flatImages = Dense(64, activation = \"relu\")(mergeImages)\n",
        "flatImages = Dense(16, activation = \"relu\")(flatImages)\n",
        "\n",
        "# merge input models\n",
        "merges = concatenate([model1.output,flatImages],name=\"concatenated_layer\")\n",
        "#flatImage = Dense(16, activation = \"relu\")(mergeImage)\n",
        "\n",
        "\n",
        "# interpretation model\n",
        "top_model = Dense(64, activation = \"relu\")(merges)\n",
        "top_model = Dropout(rate=0.3)(top_model)\n",
        "dense = Dense(16, activation = \"relu\")(top_model)\n",
        "\n",
        "zone_pred = Dense(4, activation = 'softmax', name='zone_Prediction')(dense)\n",
        "plus_pred = Dense(1, activation = 'sigmoid', name='Plus_Prediction')(dense)\n",
        "stage_pred = Dense(5, activation = 'softmax', name='stage_Prediction')(dense)\n",
        "\n",
        "model = Model(inputs=[model1.input, conv_base2.input, conv_base3.input], outputs=[stage_pred,zone_pred,plus_pred])\n",
        "# summarize layers\n",
        "print(model.summary())\n",
        "\n",
        "# plot model\n",
        "plot_model(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "5q40k7t2hBeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_bincrossentropy(true, pred, weight_zero = 1, weight_one = 1):\n",
        "    \"\"\"\n",
        "    Calculates weighted binary cross entropy. The weights are fixed.\n",
        "        \n",
        "    This can be useful for unbalanced catagories.\n",
        "    \n",
        "    Adjust the weights here depending on what is required.\n",
        "    \n",
        "    For example if there are 10x as many positive classes as negative classes,\n",
        "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
        "        will be penalize 10 times as much as false negatives.\n",
        "    \"\"\"\n",
        "    true = tf.cast(true, dtype=tf.float32)\n",
        "    # calculate the binary cross entropy\n",
        "    bin_crossentropy = ke.binary_crossentropy(true, pred)\n",
        "    \n",
        "    # apply the weights\n",
        "    weights = true * weight_one + (1. - true) * weight_zero\n",
        "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
        "\n",
        "    return ke.mean(weighted_bin_crossentropy)"
      ],
      "metadata": {
        "id": "s9Hb-oz-4JkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss={\n",
        "                  'zone_Prediction': 'categorical_crossentropy', \n",
        "                  'stage_Prediction': 'categorical_crossentropy', \n",
        "                  'Plus_Prediction': weighted_bincrossentropy},\n",
        "              metrics={\n",
        "                  'zone_Prediction': 'accuracy', \n",
        "                  'stage_Prediction': 'accuracy',\n",
        "                  'Plus_Prediction': 'accuracy'})"
      ],
      "metadata": {
        "id": "59y2TNIBwKbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stage.dtype)\n",
        "print(zone.dtype)\n",
        "plus.astype(int)\n",
        "print(plus.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmAcnDVLzTA4",
        "outputId": "c10cf55b-e714-4f29-b516-fbc061610cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32\n",
            "float32\n",
            "int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss1=[]\n",
        "loss2=[]\n",
        "loss3=[]\n",
        "accuracy1=[]\n",
        "accuracy2=[]\n",
        "accuracy3=[]\n",
        "loss=[]\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "XgHnADBh-2o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "num_epochs = 50\n",
        "batch_size = 10\n",
        "optimizer_affine = tf.keras.optimizers.Adam(learning_rate=lr)  "
      ],
      "metadata": {
        "id": "1Ai44l3MS9XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48av8tKx3jsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs =10\n",
        "batch_size =10\n",
        "for epoch in range(1,num_epochs+1):\n",
        "  shape1 = FAImages.shape\n",
        "  print(shape1)\n",
        "  num_of_images = shape1[0]\n",
        "  print('num_of_images',num_of_images)\n",
        "  num_of_batches = int( num_of_images/batch_size)\n",
        "  print('num of Batches=', num_of_batches)\n",
        "  for idx in range(0,num_of_batches):\n",
        "    batch_idx = np.random.randint(num_of_images, size=batch_size)\n",
        "    FAImagesBatch = FAImages[batch_idx, :]\n",
        "    fundusImagesBatch = fundusImages[batch_idx,:]\n",
        "    ageBatch = age[batch_idx]\n",
        "    zoneBatch = zone[batch_idx]\n",
        "    stageBatch = stage[batch_idx]\n",
        "    plusBatch = plus[batch_idx]\n",
        "    medical_dataBatch = medical_data[batch_idx]\n",
        "    loss= model.train_on_batch([medical_dataBatch,FAImagesBatch,fundusImagesBatch],[stageBatch,zoneBatch,plusBatch])\n",
        "    loss1.append(loss[1])\n",
        "    loss2.append(loss[2])\n",
        "    loss3.append(loss[3])\n",
        "    accuracy1.append(loss[4])\n",
        "    accuracy2.append(loss[5])\n",
        "    accuracy3.append(loss[6])\n",
        "    if idx%10==0:\n",
        "      print('Epoch=',epoch,' Batch=',idx)\n",
        "  print('Epoch=',epoch)"
      ],
      "metadata": {
        "id": "pZpGmTxZ7Rgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "urX0fyVhfF3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('model.h5')"
      ],
      "metadata": {
        "id": "JRBxhyrvfG5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaderTest = training_image_reader('/content/drive/MyDrive/Data/Test/ROPtest3.csv', '/content/drive/MyDrive/Data/Test/Images','testing', output_shape = (224,224))\n"
      ],
      "metadata": {
        "id": "2bJ2opVP9zsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FAImages = dataloaderTest['FAImages']\n",
        "fundusImages = dataloaderTest['fundusImages']\n",
        "age = dataloaderTest['age']\n",
        "age = (age - meanAge) / varianceAge\n",
        "#print(age)\n",
        "weight = dataloaderTest['weight']\n",
        "#print(weight)\n",
        "weight = (weight-meanWeight)/(varianceWeight+1e-8)\n",
        "#print('weight')\n",
        "print(weight)\n",
        "zone = dataloaderTest['zone']\n",
        "stage = dataloaderTest['stage']\n",
        "plus = dataloaderTest['plus']\n",
        "\n",
        "medical_data = np.stack((age,weight), axis = 1)\n",
        "shape1 = FAImages.shape\n",
        "num_of_images = shape1[0]"
      ],
      "metadata": {
        "id": "7s9yEP9mF8qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_of_images)"
      ],
      "metadata": {
        "id": "9nnAmTSICM9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634c5412-85c6-4460-dca3-faeac639fb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stagePreds=[]\n",
        "zonePreds=[]\n",
        "plusPreds=[]"
      ],
      "metadata": {
        "id": "K0QLKzGHLWFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oM9ln7UyExei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "[stagePreds,zonePreds,plusPreds] = model([medical_data,FAImages,fundusImages])\n"
      ],
      "metadata": {
        "id": "5k-i-Vo4hj5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJ0iCwEaLAP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findMaxCountVal(arr):\n",
        "  counts = np.bincount(arr)\n",
        "  max_count = counts.max()\n",
        "  max_value = np.argmax(counts)\n",
        "  return [max_value,max_count]"
      ],
      "metadata": {
        "id": "eUwgEeM2Lh50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zonePreds = np.argmax(zonePreds, axis=1)\n",
        "stagePreds = np.argmax(stagePreds, axis=1)\n",
        "print(zonePreds)\n",
        "print(stagePreds)"
      ],
      "metadata": {
        "id": "R-oGoYq9jZgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e842351c-908b-41bc-f3ca-da19c273d579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stagePreds=np.array(stagePreds)\n",
        "zonePreds = np.array(zonePreds)\n",
        "plusPreds = np.array(plusPreds)\n",
        "arr_size = len(plusPreds)\n",
        "for x in range(0,arr_size):\n",
        "  if plusPreds[x]<=0.5:\n",
        "    plusPreds[x]=0\n",
        "  else:\n",
        "    plusPreds[x]=1\n",
        "plusPreds = plusPreds.astype(np.int64)\n",
        "plusPreds = plusPreds.flatten()\n",
        "print(stagePreds)\n",
        "print(plusPreds)\n",
        "print(zonePreds)\n"
      ],
      "metadata": {
        "id": "wfOL--DiZ_Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "stageVal, stageMaxCount = findMaxCountVal(stagePreds)\n",
        "zoneVal, zoneMaxCount = findMaxCountVal(zonePreds)\n",
        "plusVal, plusMaxCount = findMaxCountVal(plusPreds)\n",
        "stageValPer = (stageMaxCount/num_of_images)*100\n",
        "zoneValPer = (zoneMaxCount/num_of_images)*100\n",
        "plusValPer = (plusMaxCount/num_of_images)*100"
      ],
      "metadata": {
        "id": "HK1T4j1hbf2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('stage=',stage[0],' stagePred=',stageVal,' stageMaxCount=',stageMaxCount,' stageValPer=', stageValPer)\n",
        "print('zone=',zone[0],' zonePred=',zoneVal,' zoneMaxCount=',zoneMaxCount,' zoneValPer=', zoneValPer)\n",
        "print('plus=',plus[0],' plusPred=',plusVal,' plusMaxCount=',plusMaxCount,' plusValPer=', plusValPer)\n"
      ],
      "metadata": {
        "id": "Uq2vbtmAdJ1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad24c478-7eac-4b53-bf19-0bde1f982ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stage= [0. 0. 1. 0. 0.]  stagePred= 2  stageMaxCount= 16  stageValPer= 100.0\n",
            "zone= [0. 0. 0. 1.]  zonePred= 2  zoneMaxCount= 16  zoneValPer= 100.0\n",
            "plus= 1  plusPred= 0  plusMaxCount= 12  plusValPer= 75.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# write list to binary file\n",
        "def write_list(a_list):\n",
        "    # store list in binary file so 'wb' mode\n",
        "    with open('listfile', 'wb') as fp:\n",
        "        pickle.dump(names, fp)\n",
        "        print('Done writing list into a binary file')\n",
        "\n",
        "# Read list to memory\n",
        "def read_list():\n",
        "    # for reading also binary mode is important\n",
        "    with open('sampleList', 'rb') as fp:\n",
        "        n_list = pickle.load(fp)\n",
        "        return n_list"
      ],
      "metadata": {
        "id": "4BhFenvluvi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('loss1.pkl', 'wb') as f:\n",
        "  # Pickle the list and write it to the file\n",
        "  pickle.dump(loss1, f)"
      ],
      "metadata": {
        "id": "aYqR_tQWFVMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('loss2.pkl', 'wb') as f:\n",
        "  # Pickle the list and write it to the file\n",
        "  pickle.dump(loss2, f)"
      ],
      "metadata": {
        "id": "-En3Z4ZIF0na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('loss3.pkl', 'wb') as f:\n",
        "  # Pickle the list and write it to the file\n",
        "  pickle.dump(loss3, f)"
      ],
      "metadata": {
        "id": "_GjI4DzsF3T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('accuracy1.pkl', 'wb') as f:\n",
        "  # Pickle the list and write it to the file\n",
        "  pickle.dump(accuracy1, f)"
      ],
      "metadata": {
        "id": "iJUlGeGyF5Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('accuracy2.pkl', 'wb') as f:\n",
        "  # Pickle the list and write it to the file\n",
        "  pickle.dump(accuracy2, f)"
      ],
      "metadata": {
        "id": "F4uke6hPF-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('accuracy3.pkl', 'wb') as f:\n",
        "  # Pickle the list and write it to the file\n",
        "  pickle.dump(accuracy3, f)"
      ],
      "metadata": {
        "id": "812-ZN_8GA7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('loss1.pkl', 'rb') as f:\n",
        "  # Unpickle the data and store it in a variable\n",
        "  newloss1 = pickle.load(f)\n",
        "print(newloss1)"
      ],
      "metadata": {
        "id": "aMJfxAxxGDAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "g3M323ugGMkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_array(array1) :\n",
        "  n =len(array1)\n",
        "  idx=0\n",
        "  newArray=[]\n",
        "  while(idx<n):\n",
        "    if(idx%20==0):\n",
        "     newArray.append(array1[idx])\n",
        "    idx =idx+1\n",
        "  return newArray"
      ],
      "metadata": {
        "id": "6svAaWPKbS4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy11 = new_array(np.array(accuracy1))\n",
        "accuracy22 = new_array(np.array(accuracy2))\n",
        "accuracy33 = new_array(np.array(accuracy3))"
      ],
      "metadata": {
        "id": "YQJGuthubaVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot each list as a line\n",
        "ax.plot(range(len(accuracy11)), accuracy11, label='Stage accuracy')\n",
        "ax.plot(range(len(accuracy22)), accuracy22, label='Zone accuracy')\n",
        "ax.plot(range(len(accuracy33)), accuracy33, label='Plus accuracy')\n",
        "\n",
        "# Add a legend and show the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c9g3-9GWauND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataTest = pd.read_csv('/content/drive/MyDrive/Data/Data/ROPTrainingData.csv')\n",
        "mrnNumbers = dataTest.iloc[:,0]\n",
        "num_of_mrns = len(mrnNumbers)\n",
        "eyePosition =  dataTest.iloc[:,1]\n",
        "for idx in range(0,num_of_mrns):\n",
        "  mrnNumber = mrnNumbers[idx]\n",
        "  dataloader = testing_image_reader(mrnNumber,'/content/drive/MyDrive/Data/Data/ROPTrainingData.csv', '/content/drive/MyDrive/Data/Data/Images','training', output_shape = (224,224))\n",
        "  FAImages = dataloaderTest['FAImages']\n",
        "  fundusImages = dataloaderTest['fundusImages']\n",
        "  age = dataloaderTest['age']\n",
        "  age = (age - meanAge) / varianceAge\n",
        "  #print(age)\n",
        "  weight = dataloaderTest['weight']\n",
        "  #print(weight)\n",
        "  weight = (weight-meanWeight)/(varianceWeight+1e-8)\n",
        "  #print('weight')\n",
        "  print(weight)\n",
        "  zone = dataloaderTest['zone']\n",
        "  stage = dataloaderTest['stage']\n",
        "  plus = dataloaderTest['plus']\n",
        "\n",
        "  medical_data = np.stack((age,weight), axis = 1)\n",
        "  shape1 = FAImages.shape\n",
        "  num_of_images = shape1[0]\n",
        "  print(num_of_images)\n",
        "  stagePreds=[]\n",
        "  zonePreds=[]\n",
        "  plusPreds=[]\n",
        "  [stagePreds,zonePreds,plusPreds] = model([medical_data,FAImages,fundusImages])\n",
        "  zonePreds = np.argmax(zonePreds, axis=1)\n",
        "  stagePreds = np.argmax(stagePreds, axis=1)\n",
        "  stagePreds=np.array(stagePreds)\n",
        "  zonePreds = np.array(zonePreds)\n",
        "  plusPreds = np.array(plusPreds)\n",
        "  arr_size = len(plusPreds)\n",
        "  for x in range(0,arr_size):\n",
        "    if plusPreds[x]<=0.5:\n",
        "      plusPreds[x]=0\n",
        "    else:\n",
        "      plusPreds[x]=1\n",
        "  plusPreds = plusPreds.astype(np.int64)\n",
        "  plusPreds = plusPreds.flatten()\n",
        "  print('mrnNumber=',mrnNumber)\n",
        "  print('eyePosition=',eyePosition[idx])\n",
        "  print(stagePreds)\n",
        "  print(plusPreds)\n",
        "  print(zonePreds)\n",
        "  stageVal, stageMaxCount = findMaxCountVal(stagePreds)\n",
        "  zoneVal, zoneMaxCount = findMaxCountVal(zonePreds)\n",
        "  plusVal, plusMaxCount = findMaxCountVal(plusPreds)\n",
        "  stageValPer = (stageMaxCount/num_of_images)*100\n",
        "  zoneValPer = (zoneMaxCount/num_of_images)*100\n",
        "  plusValPer = (plusMaxCount/num_of_images)*100\n",
        "  print('stage=',stage[0],' stagePred=',stageVal,' stageMaxCount=',stageMaxCount,' stageValPer=', stageValPer)\n",
        "  print('zone=',zone[0],' zonePred=',zoneVal,' zoneMaxCount=',zoneMaxCount,' zoneValPer=', zoneValPer)\n",
        "  print('plus=',plus[0],' plusPred=',plusVal,' plusMaxCount=',plusMaxCount,' plusValPer=', plusValPer)\n"
      ],
      "metadata": {
        "id": "3Vbu_5ClbwWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_image_reader(mrnNum,csv_file, image_dir, trainingCond,output_shape = (224,224)):\n",
        "    data = pd.read_csv(csv_file)\n",
        "    mrnNumbers = data.iloc[:,0]\n",
        "    eyePosition =  data.iloc[:,1]\n",
        "    gestational_Age = data.iloc[:,3]\n",
        "    weights = data.iloc[:,4]\n",
        "    zoneValue = data.iloc[:,6]\n",
        "    #print('zoneValue')\n",
        "    #print(zoneValue)\n",
        "   # print('end')\n",
        "    stageValue = data.iloc[:,7]\n",
        "    PlusValue = data.iloc[:,8]\n",
        "    \n",
        "    num_of_mrns = len(mrnNumbers)\n",
        "    \n",
        "    FAImages = []\n",
        "    fundusImages = []\n",
        "    age = []\n",
        "    weight = []\n",
        "\n",
        "    zone = []\n",
        "    stage= []\n",
        "    plus= []\n",
        "    maxm =0;\n",
        "    for idx in range(0,num_of_mrns):\n",
        "      if(mrnNumbers[idx] == mrnNum):\n",
        "\n",
        "        pathFA = str(mrnNumbers[idx])+'/'+'FALeft'\n",
        "        pathFundus = str(mrnNumbers[idx])+'/'+'fundusLeft'\n",
        "        if eyePosition[idx]=='R':\n",
        "         pathFA = str(mrnNumbers[idx])+'/'+'FARight'\n",
        "         pathFundus = str(mrnNumbers[idx])+'/'+'fundusRight'\n",
        "        pathFA = os.path.join(image_dir,pathFA)\n",
        "        pathFundus = os.path.join(image_dir,pathFundus)\n",
        "       #print(pathFA)\n",
        "       #print(pathFundus)\n",
        "        for images1 in os.listdir(pathFA):\n",
        "           #if maxm == 10:\n",
        "            #break\n",
        "           if (images1.endswith(\".jpg\")):\n",
        "             for images2 in os.listdir(pathFundus):\n",
        "               #if maxm == 10:\n",
        "               # break\n",
        "               if (images2.endswith(\".jpg\")):\n",
        "                #print(images1)\n",
        "                #print(images2)\n",
        "                 pathImages1 = os.path.join(pathFA+'/'+images1)\n",
        "                 pathImages2 = os.path.join(pathFundus+'/'+images2)\n",
        "                #print(pathImages1)\n",
        "                #print(pathImages2)\n",
        "                 faImage = cv2.imread(pathImages1)\n",
        "                 fundusImage = cv2.imread(pathImages2)\n",
        "                #print(faImage)\n",
        "                #print(fundusImage)\n",
        "                 faImage1 = square_padding(faImage)\n",
        "                 faImage1 = cv2.resize(faImage, output_shape)\n",
        "                 FAImages.append(faImage1)\n",
        "                 fundusImage1 = square_padding(fundusImage)\n",
        "                 fundusImage1 = cv2.resize(fundusImage1, output_shape)\n",
        "                 fundusImages.append(fundusImage1)\n",
        "                 age.append(gestational_Age[idx])\n",
        "                 weight.append(weights[idx])\n",
        "                 zone.append(zoneValue[idx])\n",
        "                 stage.append(stageValue[idx])\n",
        "                 plus.append(PlusValue[idx])  \n",
        "                 maxm = maxm+1              \n",
        "                                            \n",
        "    FAImages = np.array(FAImages)\n",
        "    FAImages =FAImages/255\n",
        "    fundusImages = np.array(fundusImages)\n",
        "    fundusImages = fundusImages/255\n",
        "    age = np.array(age)\n",
        "    weight = np.array(weight)\n",
        "    zone = np.array(zone)\n",
        "    \n",
        "    #print('zoneee')\n",
        "    #print(zone)\n",
        "    #print('endl')\n",
        "    stage = np.array(stage)\n",
        "    \n",
        "    #if trainingCond == 'training':\n",
        "    zone = to_categorical(zone, num_classes=4)\n",
        "    stage = to_categorical(stage, num_classes=5)\n",
        "   # print('zoneee')\n",
        "   # print(zone)\n",
        "   # print('endl')\n",
        "    \n",
        "    plus = np.array(plus)\n",
        "   # print(stage.shape)\n",
        "    dataset = {'FAImages': FAImages, 'fundusImages': fundusImages, 'age':age, 'weight': weight, 'zone':zone, 'stage':stage, 'plus':plus}  \n",
        "    print(len(plus))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Dk8uC_BMMYgb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}